#!/usr/bin/env ruby

# Script to migrate Firebase projects to DatablockStorage en-masse
# takes a file with a channel_id per line, and migrates them.
#
# Usage:
# bundle exec bin/migrate_firebase_projects firebase-channel-ids.txt
#
# This will create multiple files that log success/failure/skips:
# - firebase-channel-ids.txt.success
# - firebase-channel-ids.txt.fail
# - firebase-channel-ids.txt.exceptions
# - firebase-channel-ids.txt.skipped
#
# Using these files, if the script is interrupted, it can be restarted
# and will not attempt to re-do any already processed channel_ids.
# Multiple copies of the script should not be run at once.
#
# Once migration to DatablockStorage is complete, the script should be deleted.
#
# TODO: post-firebase-cleanup, remove this script #56994

# If DRY_RUN: run SQL in a transaction but rollback before committing
DRY_RUN = false

# De-duplicate tables: see if the records exactly match an existing shared
# table and link it to that using is_shared_table instead of copying the records
FIND_SHARED_TABLES = true

# Don't import a project_id if it doesn't exist in the Projects table
# this keeps us from migrating data from already deleted projects. Instead
# log the channel_id to the .skipped file.
#
# Probably want `SKIP_MISSING_PROJECT_IDS = true` for the real production run,
# but when running locally, you'll probably want it set to false (since you don't
# have a real Projects table locally).
SKIP_MISSING_PROJECT_IDS = true

# DEBUG FLAGS:
DEBUG_SHARED_TABLES = false
# Limit number of items printed to N to avoid swamping output, use -1 to not limit
DEBUG_SHARED_TABLES_LIMIT_PRINTS_TO = 2

if DRY_RUN
  puts "DRY RUN: no changes will be committed to the database"
else
  puts
  puts "Not a dry run, DB WILL BE MODIFIED: press ctrl-c to abort"
  sleep(2) # give 2 seconds to cancel if you meant to do a DRY_RUN
end

require 'firebase'
require 'json'

# hashdiff gem is only importable in dev/test environemnts
require 'hashdiff' if DEBUG_SHARED_TABLES

require_relative '../config/environment'

def set_active_record_connection_pool_size(pool_size)
  ActiveRecord::Base.connection_pool.disconnect!
  ActiveRecord::Base.establish_connection(
    ActiveRecord::Base.configurations[Rails.env].merge('pool' => pool_size)
  )
end

set_active_record_connection_pool_size(10)

# Don't use more workers than we have connections in our SQL connection pool.
# NUM_PARALLEL_WORKERS = 1
NUM_PARALLEL_WORKERS = ActiveRecord::Base.connection_pool.size - 1

def get_project_id(channel_id)
  storage_decrypt_channel_id(channel_id)[1]
end

SHARED_TABLE_NAMES = Set.new DatablockStorageTable.get_shared_table_names
SHARED_TABLES = SHARED_TABLE_NAMES.index_with do |table_name|
  Set.new DatablockStorageTable.find_shared_table(table_name).read_records
end

def print_diff(set1, set2)
  limit_prints = lambda {|arr| arr[0...DEBUG_SHARED_TABLES_LIMIT_PRINTS_TO]}

  truncate = lambda {|str, length = 120| str.length > length ? str[0, length - 3] + "..." : str}

  hash1 = set1.index_by {|item| item["id"]}
  hash2 = set2.index_by {|item| item["id"]}
  all_ids = (hash1.keys + hash2.keys).uniq

  only_in_set1 = []
  only_in_set2 = []
  in_both_but_different = []
  all_ids.each do |id|
    if hash1[id].nil?
      only_in_set2 << hash2[id]
    elsif hash2[id].nil?
      only_in_set1 << hash1[id]
    elsif hash1[id] != hash2[id]
      in_both_but_different << [hash1[id], hash2[id]]
    end
  end

  limit_prints.call(only_in_set1).each do |item|
    puts "  ID only in shared_table  : #{truncate.call(item.to_s)}"
  end

  limit_prints.call(only_in_set2).each do |item|
    puts "  ID only in table         : #{truncate.call(item.to_s)}"
  end

  limit_prints.call(in_both_but_different).each do |item1, item2|
    diff = Hashdiff.diff(item1, item2)
    puts "  different values for ID  :"
    limit_prints.call(diff).each do |change|
      if change[0] == '~'
        # ~ change means the value is in both sets, but its different
        puts "    ~#{change[1]}:"
        puts "      -#{change[2]}"
        puts "      +#{change[3]}"
      else
        # Alternative is a +/- change, which means the key is only in one set
        puts "    #{change}"
      end
    end
  end
end

# notably doesn't return a bool, instead returns the shared table name or NOT_SHARED
NOT_SHARED = nil
def get_is_shared_table(table_name, records)
  unless SHARED_TABLE_NAMES.include? table_name
    puts "NOT SHARED table_name: #{table_name}" if DEBUG_SHARED_TABLES
    return NOT_SHARED
  end

  shared_table_records = SHARED_TABLES[table_name]

  unless shared_table_records.length == records.length
    puts "NOT SHARED length    : #{table_name}" if DEBUG_SHARED_TABLES
    return NOT_SHARED
  end

  unless shared_table_records == Set.new(records)
    puts "NOT SHARED records   : #{table_name}" if DEBUG_SHARED_TABLES
    print_diff(shared_table_records, Set.new(records)) if DEBUG_SHARED_TABLES
    return NOT_SHARED
  end

  puts "SHARED               : #{table_name}" if DEBUG_SHARED_TABLES
  table_name
end

def fetch_datablock_tables(channel, project_id)
  datablock_tables = []
  tables = channel.dig("metadata", "tables") || {}

  # Some data in firebase is corrupted, e.g. has an array instead of a map for tables
  return {} unless tables.is_a? Hash

  tables.each do |table_name, table_data|
    columns = table_data["columns"].values.map {|col| col["columnName"]}
    json_records = channel.dig("storage", "tables", table_name, "records") || []

    if json_records.is_a? Hash
      # Most storage.tables.TABLE_NAME.records are arrays [record_json,...], but some of
      # them are stored as objects/hashes instead ({record_id => record_json, ...}).
      json_records = json_records.values
    end

    records = json_records.compact.map {|record| JSON.parse(record)}

    is_shared_table = FIND_SHARED_TABLES ? get_is_shared_table(table_name, records) : nil

    datablock_table = {
      project_id: project_id,
      table_name: table_name,
      columns: columns,
      is_shared_table: is_shared_table,
      created_at: Time.now,
      updated_at: Time.now
    }

    # if the table is a shared table, we don't need to store the records
    datablock_records = is_shared_table ? [] :
      records.map do |record|
        {
          project_id: project_id,
          table_name: table_name,
          record_id: record["id"],
          record_json: record
        }
      end

    datablock_tables << {table: datablock_table, records: datablock_records}
  end

  datablock_tables
end

def fetch_datablock_kvps(channel, project_id)
  kvps = channel.dig("storage", "keys") || []
  kvps.reject {|key, value| key.nil? || value.nil?}.map do |key, value|
    {
      project_id: project_id,
      key: key,
      value: JSON.parse(value)
    }
  rescue => exception
    exception.message << " (trying to parse: #{value.inspect})"
    raise
  end
end

def insert_datablock_tables(tables)
  tables.each do |table|
    DatablockStorageTable.create!(table[:table])
    DatablockStorageRecord.insert_all!(table[:records]) unless table[:records].empty?
  end
end

def insert_datablock_kvps(kvps)
  kvps.each do |kvp|
    DatablockStorageKvp.create!(kvp)
  end
end

def firebase_get(path)
  base_uri = "https://#{CDO.firebase_name}.firebaseio.com/"
  firebase_secret = ENV['FIREBASE_SECRET'] || CDO.firebase_secret
  raise "FIREBASE_SECRET not defined" unless firebase_secret
  firebase = Firebase::Client.new base_uri, firebase_secret
  response = firebase.get(path)
  raise "Error fetching #{path} from Firebase: #{response.code}" unless response.success?
  response.body
end

def firebase_get_channel(channel_id)
  firebase_get("/v3/channels/#{channel_id}")
end

class SkippedMissingProject < StandardError
end

def migrate(channel_id)
  channel = firebase_get_channel(channel_id)
  project_id = get_project_id(channel_id)

  if SKIP_MISSING_PROJECT_IDS && !Project.exists?(project_id)
    # Don't import project_ids that don't exist in the Projects table, they
    # could have been deleted by the user, but never cleaned up from Firebase
    raise SkippedMissingProject
  end

  tables = fetch_datablock_tables(channel, project_id)
  kvps = fetch_datablock_kvps(channel, project_id)
  ActiveRecord::Base.transaction do
    insert_datablock_tables(tables)
    insert_datablock_kvps(kvps)

    # Now that its migrated, switch the project over to using DatablockStorage
    ProjectUseDatablockStorage.create!(project_id: project_id, use_datablock_storage: true)

    # Do not allow transaction to complete if DRY_RUN==true
    raise ActiveRecord::Rollback if DRY_RUN
  end
end

def success_filename(log_filename_prefix)
  "#{log_filename_prefix}.success"
end

def fail_filename(log_filename_prefix)
  "#{log_filename_prefix}.fail"
end

def skipped_filename(log_filename_prefix)
  "#{log_filename_prefix}.skipped"
end

METRICS_LOGGING_INTERVAL_S = 60
class MetricsTracker
  def initialize(total_count = 0, failed_count = 0, succeeded_count = 0, skipped_count = 0)
    @start_time = Time.now
    @last_time = @start_time

    @succeeded_count = 0
    @failed_count = 0
    @skipped_count = 0

    @total_count = total_count
    @total_failed_count = failed_count
    @total_succeeded_count = succeeded_count
    @total_skipped_count = skipped_count

    # First logging interval is shorter than METRICS_LOGGING_INTERVAL_S
    # to give immediate feedback on rate
    @logging_interval_s = 5

    if failed_count || succeeded_count
      puts "Resuming from previous run: #{win_lose_string}"
    end
  end

  def failed
    @failed_count += 1
    print "❌" unless DEBUG_SHARED_TABLES
  end

  def succeeded
    @succeeded_count += 1
    print "🟢" unless DEBUG_SHARED_TABLES
  end

  def skipped
    @skipped_count += 1
    print "·" unless DEBUG_SHARED_TABLES
  end

  def percent_complete
    total_processed = @total_succeeded_count + @total_failed_count
    percent = (total_processed.to_f / @total_count) * 100
    format("%.2f", percent) + "%"
  end

  def win_lose_string
    "#{@total_succeeded_count} succeeded, #{@total_failed_count} failed, #{@total_skipped_count} skipped, #{percent_complete} complete"
  end

  def print_metrics
    now = Time.now
    elapsed = now - @last_time
    # if its been more than N seconds, we print metrics...
    if elapsed > @logging_interval_s
      @logging_interval_s = METRICS_LOGGING_INTERVAL_S
      processed = @succeeded_count + @failed_count
      processing_rate = processed / elapsed

      @total_succeeded_count += @succeeded_count
      @total_failed_count += @failed_count
      @total_skipped_count += @skipped_count

      total_processed = @total_succeeded_count + @total_failed_count + @total_skipped_count

      seconds_remaining = (@total_count - total_processed) / processing_rate
      days_remaining = format('%.2f', seconds_remaining / 86400)

      puts
      puts "migrated #{total_processed} of #{@total_count}, #{format('%.1f', processing_rate)} channels/s, ~#{days_remaining} days remaining, #{win_lose_string}"
      puts

      # Reset metrics for next sampling period
      @succeeded_count = 0
      @failed_count = 0
      @last_time = now
    end
  end
end

def stream_results_to_logs(log_filename_prefix, pool, results, count, failed_count, succeeded_count, skipped_count)
  succeeded_log = File.open(success_filename(log_filename_prefix), 'a+')
  succeeded_log.sync = true

  failed_log = File.open(fail_filename(log_filename_prefix), 'a+')
  failed_log.sync = true

  exception_log = File.open("#{log_filename_prefix}.exceptions", 'a+')
  exception_log.sync = true

  skipped_log = File.open(skipped_filename(log_filename_prefix), 'a+')
  skipped_log.sync = true

  metrics_tracker = MetricsTracker.new(count, failed_count, succeeded_count, skipped_count)

  log_results = -> do
    loop do
      channel_id, success, exception = results.pop(non_block: true)

      if success
        metrics_tracker.succeeded
        succeeded_log.puts(channel_id)
      elsif exception.is_a? SkippedMissingProject
        metrics_tracker.skipped
        skipped_log.puts(channel_id)
      else
        metrics_tracker.failed
        failed_log.puts(channel_id)
        exception_log.puts("Exception migrating #{channel_id}, https://console.firebase.google.com/project/cdo-v3-prod/database/cdo-v3-prod/data/~2Fv3~2Fchannels~2F#{channel_id}")
        exception_log.puts("#{exception.class}: #{exception.message}")
        exception_log.puts(exception.backtrace)
        exception_log.puts
      end
    end
  rescue ThreadError
    # queue.pop(non_block: true) raises ThreadError if the queue is empty
  end

  until pool.shutdown?
    log_results.call
    metrics_tracker.print_metrics
    sleep 0.1
  end
  puts "pool shutdown, draining results queue"
  log_results.call
ensure
  succeeded_log.close
  failed_log.close
  exception_log.close
end

def migrate_all(channel_ids, log_filename_prefix: "firebase-channel-ids.txt", count: 0, failed_count: 0, succeeded_count: 0, skipped_count: 0)
  puts "Running with #{NUM_PARALLEL_WORKERS} parallel workers"

  pool = Concurrent::FixedThreadPool.new(NUM_PARALLEL_WORKERS)

  results = Queue.new

  puts "Building task queue..."
  channel_ids.each do |channel_id|
    pool.post do
      migrate(channel_id)
      results.push [channel_id, true]
    rescue => exception
      results.push [channel_id, false, exception]
    end
  end
  puts "done building task queue, starting migration"
  puts

  pool.shutdown
  stream_results_to_logs(log_filename_prefix, pool, results, count, failed_count, succeeded_count, skipped_count)
  pool.wait_for_termination
ensure
  # This is particularly useful because ctrl-c from within irb
  # doesn't kill the thread pool, which makes repl-driven dev hard.
  unless pool.shutdown?
    puts "migrate_all() interrupted, killing thread pool"
    pool.shutdown
    pool.kill
    pool.wait_for_termination
  end
end

def load_channel_ids(filename)
  Set.new(File.readlines(filename).map(&:strip))
rescue Errno::ENOENT
  puts "Warning: couldn't read #{filename}"
  Set.new
end

def migrate_from_file(filename)
  puts "Migrating channel_ids from #{filename}"
  channel_ids = load_channel_ids(filename)
  failed_channel_ids = load_channel_ids(fail_filename(filename))
  skipped_channel_ids = load_channel_ids(skipped_filename(filename))
  succeeded_channel_ids = load_channel_ids(success_filename(filename))

  puts "Num Total channel_ids: #{channel_ids.length}"
  puts "Num Failed channel_ids: #{failed_channel_ids.length}"
  puts "Num Skipped channel_ids: #{skipped_channel_ids.length}"
  puts "Num Succeeded channel_ids: #{succeeded_channel_ids.length}"
  puts

  count = channel_ids.length
  failed_count = failed_channel_ids.length
  succeeded_count = succeeded_channel_ids.length
  skipped_count = skipped_channel_ids.length

  # Don't re-process channel_ids that have already been processed
  # no matter whether they failed, succeeded, or were skipped
  channel_ids -= failed_channel_ids
  channel_ids -= succeeded_channel_ids
  channel_ids -= skipped_channel_ids

  puts "Num channel_ids left to process: #{channel_ids.length}"
  migrate_all(channel_ids, log_filename_prefix: filename, count: count, failed_count: failed_count, succeeded_count: succeeded_count, skipped_count: skipped_count)
end

if $PROGRAM_NAME == __FILE__
  id_filename = ARGV[0] || 'firebase-channel-ids.txt'
  migrate_from_file(id_filename)
end
