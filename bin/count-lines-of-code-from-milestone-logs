#!/usr/bin/env ruby
#
# This script downloads milestone logs from our S3 bucket, counts the lines of code documented
# in the logs, and stores the total lines (per instance, per log, per day) in
# pegasus/cache/milestone-cache.json.
#
# Logs for days that are over are .gz compressed in S3. The log for the current day is not
# compressed and updates are uploaded hourly. See upload-logs-to-s3 for details on the upload
# side.
#
require_relative '../deployment'
require 'cdo/aws/s3'

def count_lines_of_code(i)
  filename = File.basename(i.key)
  extname = File.extname(filename)
  path = pegasus_dir 'cache', filename
  IO.write path, i.value

  count_command = "cut -f10 | awk '{s+=$1} END {print s}'"

  case extname
  when '.gz'
    count = `gunzip -c #{path} | #{count_command}`.to_i
  when '.log'
    count = `cat #{path} | #{count_command}`.to_i
  else
    raise ArgumentError, "Don't know how to process #{extname} files."
  end

  FileUtils.rm path

  count
end

def main
  cache_file = pegasus_dir('cache','milestone-cache.json')
  cache = JSON.parse(IO.read(cache_file)) if File.file?(cache_file)
  cache ||= {}

  AWS::S3::connect!

  marker = nil
  total = 0

  loop do
    objects = AWS::S3::Bucket.objects('cdo-logs', marker: marker, prefix: 'hosts')
    break if objects.empty?

    objects.each do |i|
      match = i.key.match /^hosts\/(?<host>[^\/]+)\/dashboard\/milestone.log/
      next unless match

      host = match[:host]
      next if ['console', 'daemon', 'production-daemon', 'staging', 'test', 'levelbuilder-staging', 'levelbuilder-development', 'development', 'react'].include? host

      cache[i.key] = count_lines_of_code(i) unless cache.has_key?(i.key) && File.extname(i.key) == '.gz'

      IO.write pegasus_dir('cache','milestone-cache.json'), JSON.pretty_generate(cache)

      total += cache[i.key]
      #puts "[#{total}] #{cache[i.key]} in #{i.key}"
    end
    marker = objects.last.key
  end

  puts total
end

main
