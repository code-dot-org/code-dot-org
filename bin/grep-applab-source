#!/usr/bin/env ruby
require_relative '../deployment'
require_relative '../lib/cdo/aws/s3'
require_relative '../shared/middleware/helpers/storage_id'
require 'cdo/db'
require 'active_support/time'

MAX_THREADS = 200
# Log to the command line every time this many source files are processed.
# Must be a multiple of MAX_THREADS.
LOG_INCREMENT = 1000

if ARGV.length < 2 || ARGV.length.odd?
  $stderr.puts "usage: nohup #{$0} <regex1> <output1>.tsv [<regex2> <output2>.tsv [...]]  > nohup-out.txt &"
  exit(1)
end

# Searches all applab source code for each regex and prints results to a corresponding tsv file.
# This is useful for seeing if we will break anyone's program if we change how a block works.
# Multiple regex-filename pairs can be specified to avoid having to run the script multiple times.
#
# The 'match' field is either the entire matched string, or an array of capture groups if any capture
# groups are specified (see String.scan docs for details).
#
# With 200 threads on production-console, this script takes approximately 1 hour to
# find and process 500K applab projects.

# a list of pairs of regexes and filenames, e.g. [[regex1, filename1], [regex2, filename2]]
commands = ARGV.map(&:to_s).map(&:strip).each_slice(2).to_a.map do |regex, filename|
  # print tsv headers
  File.open(filename, 'w') do |file|
    file << %w{channel_id owner_storage_id match project_url created_at updated_at}.join("\t") + "\n"
  end

  # construct regex
  [Regexp.new(regex), filename]
end

s3 = Aws::S3::Client.new
bucket = CDO.sources_s3_bucket
base_dir = CDO.sources_s3_directory
count = 0
key_not_found = 0

# set this to a value like 1.year.ago or '2016-10-15' to search only recent
# projects. This can be useful to search the remaining projects if the first run
# dies part way through.
created_after = nil

puts "[#{Time.now}] Fetching a list of applab apps from Pegasus DB. This step may take approximately 20 minutes"
puts "to run over ~15M rows in storage_apps in production..."

PEGASUS_REPORTING_DB_READONLY = sequel_connect CDO.pegasus_reporting_db_reader, CDO.pegasus_reporting_db_reader

# A array with each row representing an applab app.
applab_channels = PEGASUS_REPORTING_DB_READONLY[:storage_apps].
  where(project_type: ['applab', nil]).
  where(state: 'active').
  where {created_after.nil? || created_at > created_after}.
  grep(:value, '%applab%').
  to_a

puts "[#{Time.now}] Scanning #{applab_channels.length} source files in S3 using #{MAX_THREADS} threads..."

applab_channels.each_slice(MAX_THREADS) do |chunk|
  threads = []
  chunk.each do |row|
    owner_storage_id = row[:storage_id]
    channel_id = row[:id]
    created_at = row[:created_at]
    updated_at = row[:updated_at]

    threads << Thread.new do
      s3_filename = "#{base_dir}/#{owner_storage_id}/#{channel_id}/main.json"
      begin
        body = s3.get_object(bucket: bucket, key: s3_filename)[:body].read
      rescue Aws::S3::Errors::NoSuchKey
        key_not_found += 1
        next
      end

      source = JSON.parse(body)['source'] if body
      if source
        commands.each do |regex, filename|
          source.scan(regex).each do |match|
            channel = storage_encrypt_channel_id(owner_storage_id, channel_id)
            project_url = "#{CDO.default_scheme}#{CDO.studio_url}/projects/applab/#{channel}/view"

            File.open(filename, 'a') do |file|
              file.flock(File::LOCK_EX)
              file << "#{channel_id}\t#{owner_storage_id}\t#{match}\t#{project_url}\t#{created_at}\t#{updated_at}\n"
              file.flock(File::LOCK_UN)
            end
          end
        end
      end
    end
  end
  threads.each(&:join)

  count += chunk.length
  puts "#{count} files scanned..." if count % LOG_INCREMENT == 0
end
puts "[#{Time.now}] #{count} files scanned total"
puts "#{key_not_found} keys were not found in S3" if key_not_found
