#!/usr/bin/env ruby

require 'csv'
require_relative '../../dashboard/config/environment'

def get_substring_in_between(text, begin_string, end_string)
  text.split(begin_string).last.split(end_string).first
end

def valid_json?(string)
  !!JSON.parse(string)
rescue JSON::ParserError
  false
end

def get_blocked_by_details(blocked_by, details)
  case blocked_by
  when "comprehend"
    return {
      flagged_text: details[:flagged_segment],
      toxicity: details[:toxicity],
      max_category: details[:max_category][:name],
      score: details[:max_category][:score]
    }
  when "blocklist"
    return {flagged_text: details[:blocked_word]}
  when "webpurify"
    return {flagged_text: details[:content]}
  else
    return {}
  end
end

def main
  options = {}
  OptionParser.new do |opts|
    opts.on('--section_id SECTION_ID') do |section_id|
      options[:section_id] = Integer(section_id)
    end
  end.parse!

  CSV.open("aichat_requests.csv", "w") do |csv|
    headers = %w[
      aichat_request_id user_id username name path created_at updated_at role chat_message status toxicity max_category score blocked_by flagged_text
    ]
    csv << headers

    student_users_in_section = Section.find(options[:section_id]).followers.map(&:student_user)
    teacher_users_in_section = Section.find(options[:section_id]).active_section_instructors.map(&:instructor)
    all_users_in_section = student_users_in_section + teacher_users_in_section
    rows = AichatRequest.where(
      user: all_users_in_section,
    )

    rows.each do |row|
      user = row.user
      user_message = JSON.parse(row.new_message)
      response = row.response
      user_message_text = user_message["chatMessageText"]
      execution_status = row.execution_status

      level = Level.find(row.level_id)
      path = ""

      script_level = level.script_levels.find_by_script_id(row.script_id)
      if script_level
        # This handles chat requests that occurred on standard levels in a progression
        path = script_level.path
      else
        # This handles chat requests that occurred on a sublevel
        parent_levels = BubbleChoice.parent_levels(level.name)
        parent_levels_in_script = parent_levels.filter do |pl|
          pl.script_levels.any? {|sl| sl.script_id == row.script_id}
        end

        if parent_levels_in_script
          parent_level = parent_levels_in_script.first
          sublevel_position = parent_level.sublevel_position(level)
          path = parent_level.build_script_level_path(
            parent_level.script_levels.first,
            {sublevel_position: sublevel_position}
          )
        end
      end

      if execution_status == SharedConstants::AI_REQUEST_EXECUTION_STATUS[:USER_PROFANITY]
        # Parse response for comprehend/blocklist/webpurify details for user message.
        # There is no model response recorded if user message was flagged.
        # After https://github.com/code-dot-org/code-dot-org/pull/61095, response was updated to JSON.
        # Handle response from before and after update.
        if valid_json?(response)
          user_comprehend_response = JSON.parse(response).deep_symbolize_keys
          user_blocked_by = user_comprehend_response[:blocked_by]
          details = user_comprehend_response[:details]
          user_blocked_by_details = get_blocked_by_details(user_blocked_by, details)
          user_toxicity = user_blocked_by_details[:toxicity]
          user_max_category = user_blocked_by_details[:max_category]
          user_score = user_blocked_by_details[:score]
          user_flagged_text = user_blocked_by_details[:flagged_text]
        else
          user_comprehend_response = response.delete_prefix("Profanity detected in user input: ")
          user_toxicity = get_substring_in_between(user_comprehend_response, ":toxicity=>", ",").to_f
          user_max_category = get_substring_in_between(user_comprehend_response, ":max_category=>#<struct Aws::Comprehend::Types::ToxicContent name=", ",").delete_prefix('"').delete_suffix('"')
          user_score = get_substring_in_between(user_comprehend_response, "score=", ">").to_f
        end
      elsif execution_status == SharedConstants::AI_REQUEST_EXECUTION_STATUS[:MODEL_PROFANITY]
        # Model response is recorded in addition to blocked by service details.
        # After https://github.com/code-dot-org/code-dot-org/pull/61095, response was updated to JSON.
        # Handle response from before and after update.
        if valid_json?(response)
          model_comprehend_response = JSON.parse(response).deep_symbolize_keys
          model_blocked_by = model_comprehend_response[:blocked_by]
          details = model_comprehend_response[:details]
          model_blocked_by_details = get_blocked_by_details(model_blocked_by, details)
          model_message_text = model_comprehend_response[:text]
          model_toxicity = model_blocked_by_details[:toxicity]
          model_max_category = model_blocked_by_details[:max_category]
          model_score = model_blocked_by_details[:score]
          model_flagged_text = model_blocked_by_details[:flagged_text]
        else
          model_comprehend_response = response.delete_prefix("Profanity detected in model output: ")
          model_message_text = get_substring_in_between(model_comprehend_response, ":text=>", ", :toxicity").delete_prefix('"').delete_suffix('"')
          model_toxicity = get_substring_in_between(model_comprehend_response, ":toxicity=>", ",").to_f
          model_max_category = get_substring_in_between(model_comprehend_response, ":max_category=>#<struct Aws::Comprehend::Types::ToxicContent name=", ",").delete_prefix('"').delete_suffix('"')
          model_score = get_substring_in_between(model_comprehend_response, "score=", ">").to_f
        end
      elsif execution_status == SharedConstants::AI_REQUEST_EXECUTION_STATUS[:SUCCESS]
        # Model response is recorded.
        model_message_text = response
      end
      # No model response recorded when unexpected error or user input too large.

      base_row = [
        row.id,
        row.user_id,
        user.username,
        user.name,
        path,
        row.created_at,
        row.updated_at,
      ]

      status =
        case execution_status
        when SharedConstants::AI_REQUEST_EXECUTION_STATUS[:SUCCESS] || SharedConstants::AI_REQUEST_EXECUTION_STATUS[:MODEL_PROFANITY]
          "okay"
        when SharedConstants::AI_REQUEST_EXECUTION_STATUS[:FAILURE]
          "unexpected error"
        when SharedConstants::AI_REQUEST_EXECUTION_STATUS[:USER_PROFANITY]
          "user profanity detected"
        when SharedConstants::AI_REQUEST_EXECUTION_STATUS[:USER_INPUT_TOO_LARGE]
          "user input too large"
        else
          "unknown"
        end

      # Add user message to csv.
      csv << [
        *base_row,
        'user',
        user_message_text,
        status,
        user_toxicity,
        user_max_category,
        user_score,
        user_blocked_by,
        user_flagged_text
      ]

      next if execution_status != SharedConstants::AI_REQUEST_EXECUTION_STATUS[:SUCCESS] && execution_status != SharedConstants::AI_REQUEST_EXECUTION_STATUS[:MODEL_PROFANITY]
      if execution_status == SharedConstants::AI_REQUEST_EXECUTION_STATUS[:MODEL_PROFANITY]
        status = "model profanity detected"
      end
      # Add assistant message to csv.
      csv << [
        *base_row,
        'assistant',
        model_message_text,
        status,
        model_toxicity,
        model_max_category,
        model_score,
        model_blocked_by,
        model_flagged_text
      ]
    end
  end
end

main
