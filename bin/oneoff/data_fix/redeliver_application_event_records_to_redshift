#!/usr/bin/env ruby

require_relative '../../../dashboard/config/environment'
require 'cdo/aws/s3'
require 'optparse'
require 'json'
require 'uri'
require 'date'
require 'csv'
require 'cdo/redshift'
require 'aws-sdk-sts'

# This script redelivers application event ('firehose') event records to Redshift from the Intermediate S3 bucket
# where batches of records were stored by the Firehose data stream and where manifest objects identify the batches
# that succeeded delivery to Redshift and those that failed delivery to Redshift.

CDO.log = Logger.new($stdout)

options = {}
options[:actually_redeliver] = false
options[:modify_invalid] = false
options[:log_filename] = 'batch_redelivery_log.csv'
OptionParser.new do |opts|
  opts.banner = "Usage: bin/oneoff/data_fix/redeliver_application_event_records_to_redshift [options]"
  opts.on("-a", "--actually-redeliver", "Actually redeliver messages") do
    options[:actually_redeliver] = true
  end
  opts.on("-m", "--modify-invalid", "Modify invalid messages") do
    options[:modify_invalid] = true
  end
  opts.on("-l", "--log-filename FILE", "Set the log filename") do |file|
    options[:log_filename] = file
  end
  opts.on("-h", "--help", "Displays this help message") do
    puts opts
    exit
  end
end.parse!
CDO.log.info "Called with options: #{options}"

class SkipRedeliveryError < StandardError; end

# TODO: Limit the range of batches to reprocess. Until then, pick a single prefix and reprocess all matching error manifests.
ERROR_MANIFESTS_PREFIX = 'errors/manifests/2024/07/14'.freeze
# START_DATE_TIME = ???
# END_DATE_TIME = ???

FIREHOSE_STREAM = 'analysis-events'.freeze
FIREHOSE_INTERMEDIATE_BUCKET = 'firehose-analysis-events'.freeze
REDELIVER_DATA_PREFIX = 'redeliveries'.freeze
REDELIVERY_MANIFESTS_PREFIX = REDELIVER_DATA_PREFIX + '/manifests'
REDELIVERY_ERROR_MANIFESTS_PREFIX = REDELIVER_DATA_PREFIX + '/errors'
REDSHIFT_SCHEMA = 'analysis'.freeze
REDSHIFT_TABLE = 'application_events'.freeze
sts_client = Aws::STS::Client.new
ACCOUNT_ID = sts_client.get_caller_identity.account

def split_json_batch(batch_string)
  batch_string.delete_prefix('{').
    delete_suffix('}').
    split('}{').
    map {|obj| JSON.parse("{#{obj}}")}
end

event_json_schema = {
  "type" => "object",
  "properties" => {
    "created_at" => {
      "type" => "string",
      "format" => "date-time"
    },
    "environment" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study" => {
      "type" => "string",
      "maxLength" => 128
    },
    "study_group" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "device" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 1024
    },
    "uuid" => {
      "type" => ["string", "null"],
      "maxLength" => 128
    },
    "user_id" => {
      "type" => ["integer", "null"]
    },
    "script_id" => {
      "type" => ["integer", "null"]
    },
    "level_id" => {
      "type" => ["integer", "null"]
    },
    "project_id" => {
      "type" => ["string", "integer", "null"],
      "maxLength" => 128
    },
    "event" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 128
    },
    "data_int" => {
      "type" => ["integer", "null"]
    },
    "data_float" => {
      "type" => ["number", "null"]
    },
    "data_string" => {
      "type" => ["string", "null"],
      "maxLength" => 4096
    },
    "data_json" => {
      "type" => ["string", "object", "null"],
      "maxLength" => 65535
    }
  },
  "required" => ["created_at", "environment", "study", "event"]
}

class RecordValidationError < StandardError
  attr_reader :errors

  def initialize(errors)
    @errors = errors
    super("Record validation failed: #{errors.join(', ')}")
  end
end

def validate_record(record, schema, modify_invalid: false)
  errors = []

  # Validate required fields
  schema['required'].each do |field|
    next unless !record.key?(field) || record[field].nil?
    error_msg = "Missing required field: #{field}"
    errors << error_msg
    CDO.log.error(error_msg)
  end

  # Validate each property
  schema['properties'].each do |field, field_schema|
    next unless record.key?(field) && !record[field].nil?
    value = record[field]

    # Type validation
    types = Array(field_schema['type'])
    unless types.any? {|t| valid_type?(value, t)}
      error_msg = "Invalid type for #{field}: expected #{types.join(' or ')}, got #{value.class}"
      errors << error_msg
      CDO.log.error(error_msg)
    end

    # Format validation
    if field_schema['format'] == 'date-time' && types.include?('string')
      begin
        DateTime.iso8601(value)
      rescue ArgumentError
        error_msg = "Invalid date-time format for #{field}: #{value}"
        errors << error_msg
        CDO.log.error(error_msg)
      end
    end

    # Length validation and truncation
    next unless field_schema['maxLength'] && (types.include?('string') || types.include?('object'))
    string_value = value.is_a?(String) ? value : value.to_json
    byte_length = string_value.encode('UTF-8').bytesize
    next unless byte_length > field_schema['maxLength']
    error_msg = "#{field} exceeds maximum byte length of #{field_schema['maxLength']} (current: #{byte_length} bytes)"
    errors << error_msg
    CDO.log.error(error_msg)

    if modify_invalid
      truncated_value = truncate_to_byte_length(string_value, field_schema['maxLength'])
      record[field] = truncated_value.is_a?(String) ? truncated_value : JSON.parse(truncated_value)
    end
  end

  raise RecordValidationError.new(errors) if errors.any? && !modify_invalid
  return record, errors.any?
end

def truncate_to_byte_length(string, max_bytes)
  return string if string.encode('UTF-8').bytesize <= max_bytes

  truncated = string.encode('UTF-8')
  truncated = truncated[0...-1] while truncated.bytesize > max_bytes
  truncated
end

def valid_type?(value, type)
  case type
  when 'string'
    value.is_a?(String)
  when 'integer'
    value.is_a?(Integer)
  when 'number'
    value.is_a?(Numeric)
  when 'object'
    value.is_a?(Hash)
  when 'null'
    value.nil?
  else
    false
  end
end

s3_client = AWS::S3.create_client
redshift_client = RedshiftClient.instance

continuation_token = nil

BatchRedeliveryStatus = Struct.new(
  :error_manifest_object_key,           # S3 Key of Error Manifest
  :error_batch_object_key,              # S3 Key of Error Object specified in Manifest
  :error_batch_position_in_manifest,    # A single error manifest can reference more than one failed batch.
  :redelivery_batch_object_key,         # S3 Key of modified batch.
  :redelivery_manifest_object_key,      # S3 Key of redelivery manifest.
  :records_in_batch,                    # Number of records in original batch.
  :records_modified,                    # Number of invalid records that were modified for redelivery.
  :records_skipped,                     # Number of invalid records that were skipped for redelivery (not fixable, or modify option disabled).
  :redelivery_date_time,                # Date/Time redelivery to Redshift was attempted.
  :redelivery_status,                   # true / false
  :redelivery_status_reason             # Detailed error if status is false.
)

batches_processed = 0
batches_redelivered = 0
batches_failed = 0

# Log each batch that we attempt to redeliver.
CSV.open(options[:log_filename], 'a') do |batch_log_csv|
  batch_log_csv << BatchRedeliveryStatus.members if File.empty?(options[:log_filename])
  # List all Firehose error manifests matching a date prefix in batches until there are no more to list.
  loop do
    list_error_manifests_response = s3_client.list_objects_v2(
      bucket: FIREHOSE_INTERMEDIATE_BUCKET,
      prefix: ERROR_MANIFESTS_PREFIX,
      continuation_token: continuation_token
    )

    CDO.log.info "Retrieving #{list_error_manifests_response.contents.length} error manifests."
    # Iterate through each Firehose error manifest within the current batch.
    list_error_manifests_response.contents.each do |error_manifest_object|
      CDO.log.info "Error Manifest: #{error_manifest_object.key}"
      error_manifest_json = s3_client.get_object(
        bucket: FIREHOSE_INTERMEDIATE_BUCKET,
        key: error_manifest_object.key
      ).body.read
      # Iterate through each undelivered/errored Firehose batch identified in the current error manifest.
      JSON.parse(error_manifest_json)["entries"].each_with_index do |entry, position_in_error_manifest|
        batches_processed += 1
        invalid_records_skipped = 0
        invalid_records_modified = 0
        undelivered_batch_uri = URI.parse(entry["url"])
        batch_status = BatchRedeliveryStatus.new
        batch_status.error_manifest_object_key = error_manifest_object.key
        batch_status.error_batch_object_key = undelivered_batch_uri.path&.delete_prefix('/')
        batch_status.error_batch_position_in_manifest = position_in_error_manifest
        CDO.log.info "Attempting to redeliver error batch: #{batch_status.error_batch_object_key}"
        undelivered_batch = s3_client.get_object(
          bucket: undelivered_batch_uri.host,
          key: batch_status.error_batch_object_key
        ).body.read
        modified_batch = ''
        # Validate and modify each record in the failed batch.
        split_json_batch(undelivered_batch).each_with_index do |record, _position_in_batch|
          modified_record, record_validation_status = validate_record(record, event_json_schema, modify_invalid: options[:modify_invalid])
          invalid_records_modified += 1 if !record_validation_status && options[:modify_invalid]
          modified_batch += modified_record.to_json
        rescue RecordValidationError => exception
          invalid_records_skipped += 1
          CDO.log.info record
          CDO.log.info exception.message
        end
        raise SkipRedeliveryError.new('Skipping redelivery. --actually-redeliver is set to false.') unless options[:actually_redeliver]
        batch_status.redelivery_batch_object_key = REDELIVER_DATA_PREFIX + undelivered_batch_uri.path
        begin
          redelivery_manifest = {
            entries: [
              {
                url: "s3://#{FIREHOSE_INTERMEDIATE_BUCKET}/#{batch_status.redelivery_batch_object_key}",
                mandatory: true,
                meta: {
                  content_length: modified_batch.encode('UTF-8').bytesize
                }
              }
            ]
          }
          # Check if there's a manifest for successful redelivery of this batch because Redshift does not enforce
          # unique constraints and we must avoid duplicate redelivery.
          s3_client.head_object(bucket: FIREHOSE_INTERMEDIATE_BUCKET, key: REDELIVERY_MANIFESTS_PREFIX + undelivered_batch_uri.path)
          CDO.log.info "The batch #{undelivered_batch_uri.path} has already been redelivered."
          batches_failed += 1
        rescue Aws::S3::Errors::NotFound
          # Redelivery manifest does not exist, load the batch identified in the manifest into Redshift and then upload the manifest.
          s3_client.put_object(bucket: FIREHOSE_INTERMEDIATE_BUCKET, key: batch_status.redelivery_batch_object_key, body: modified_batch)
          load_batch_query = <<~SQL
            SET search_path TO #{REDSHIFT_SCHEMA};
            COPY #{REDSHIFT_TABLE} (created_at, environment, study, study_group, device, uuid, user_id, script_id, level_id, project_id, event, data_int, data_float, data_string, data_json)
            FROM 's3://#{FIREHOSE_INTERMEDIATE_BUCKET}/#{batch_status.redelivery_batch_object_key}'
            CREDENTIALS 'aws_iam_role=arn:aws:iam::#{ACCOUNT_ID}:role/redshift-s3'
            json 'auto' timeformat 'auto';
          SQL
          redshift_client.exec(load_batch_query)
          s3_client.put_object(
            bucket: FIREHOSE_INTERMEDIATE_BUCKET,
            key: REDELIVERY_MANIFESTS_PREFIX + undelivered_batch_uri.path,
            body: redelivery_manifest.to_json
          )
          batch_status.redelivery_manifest_object_key = REDELIVERY_MANIFESTS_PREFIX + undelivered_batch_uri.path
          batch_status.redelivery_date_time = DateTime.now.to_s
          batch_status.redelivery_status = true
          batches_redelivered += 1
          CDO.log.info "Successfully redelivered #{undelivered_batch_uri.path} to Redshift."
        end
      rescue RedshiftClient::PostgreSQLQueryError, Aws::Errors::ServiceError, SkipRedeliveryError, StandardError => exception
        batches_failed += 1
        CDO.log.info "Error redelivering batch: #{undelivered_batch_uri.path}"
        CDO.log.info exception.message
        s3_client.put_object(
          bucket: FIREHOSE_INTERMEDIATE_BUCKET,
          key: REDELIVERY_ERROR_MANIFESTS_PREFIX + undelivered_batch_uri.path,
          body: redelivery_manifest
        )
        batch_status.redelivery_manifest_object_key = REDELIVERY_ERROR_MANIFESTS_PREFIX + undelivered_batch_uri.path
        batch_status.redelivery_status = false
        batch_status.redelivery_status_reason = exception.message
      ensure
        batch_status.records_modified = invalid_records_modified
        batch_status.records_skipped = invalid_records_skipped
        batch_log_csv << batch_status.to_a
        batch_log_csv.flush
      end
    end
    break unless list_error_manifests_response.is_truncated
    continuation_token = list_error_manifests_response.next_continuation_token
  end
end

CDO.log.info "Script completed."
CDO.log.info "Batches Processed: #{batches_processed}"
CDO.log.info "Batches Successfully Redelivered: #{batches_redelivered}"
CDO.log.info "Batches Failed Redelivery: #{batches_failed}"
