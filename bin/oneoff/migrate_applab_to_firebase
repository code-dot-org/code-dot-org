#!/usr/bin/env ruby

require File.expand_path('../../../deployment', __FILE__)
require 'cdo/db'
require 'net/http'
require 'parallel'
require 'time'
require 'uri'

require_relative '../../shared/middleware/helpers/storage_id'
require_relative '../../shared/middleware/helpers/dynamo_table'
require_relative '../../shared/middleware/helpers/property_bag'

MAX_THREADS = 50
LOG_INCREMENT = 10_000
PEGASUS_REPORTING_DB_READONLY = sequel_connect(CDO.pegasus_reporting_db_reader, CDO.pegasus_reporting_db_reader)
# The following characters are illegal in firebase path names: .$#[]/
ILLEGAL_CHAR_REGEX = %r{[\.#\$\[\]/]}

def put_with_auth(url, data)
  uri = URI.parse("#{url}?auth=#{CDO.firebase_secret}")
  request = Net::HTTP::Put.new(uri)
  request.body = data

  response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: uri.scheme == "https") do |http|
    http.request(request)
  end

  unless response.is_a?(Net::HTTPSuccess)
    raise "Failed to PUT data at #{url} with error #{response.code} '#{response.body}'"
  end
end

def post_with_auth(url, data)
  uri = URI.parse("#{url}?auth=#{CDO.firebase_secret}")
  request = Net::HTTP::Post.new(uri)
  request.body = data

  response = Net::HTTP.start(uri.hostname, uri.port, use_ssl: uri.scheme == "https") do |http|
    http.request(request)
  end

  unless response.is_a?(Net::HTTPSuccess)
    raise "Failed to POST data at #{url} with error #{response.code} '#{response.body}'"
  end
end

# Sets the counters, metadata, and contents of a table in firebase.
#
# @param encrypted_channel_id [String]
# @param properties_data [Hash] Map from property name to JSON-encoded value
def set_firebase_properties(encrypted_channel_id, properties_data)
  #puts "set_firebase_properties #{encrypted_channel_id} #{properties_data.keys}"
  channel_url = "https://#{CDO.firebase_name}.firebaseio.com/v3/channels/#{encrypted_channel_id}"
  properties_url = "#{channel_url}/storage/keys.json"

  original_keys = properties_data.keys
  original_keys.each do |key|
    next unless ILLEGAL_CHAR_REGEX.match(key)
    # TODO(dave): remove this line and reenable this block once data blocks / data browser handle illegal characters
    raise "skipping key name containing illegal characters: #{key}"
    # legal_key = key.gsub(ILLEGAL_CHAR_REGEX, '-')
    # puts "renaming key #{key} to #{legal_key} in channel #{encrypted_channel_id}"
    # properties_data[legal_key] = properties_data[key]
    # properties_data.delete(key)
  end
  put_with_auth(properties_url, properties_data.to_json)
end

# Sets the counters, metadata, and contents of a table in firebase.
#
# @param encrypted_channel_id [String] An encrypted, base64-encoded channel id.
# @param table_name [String]
# @param table_records [Hash] Map from record id to json-encoded record
# @params table_columns [Array] Array of column names
def set_firebase_table(encrypted_channel_id, table_name, table_records, table_columns)
  #puts "set_firebase_table #{encrypted_channel_id} #{table_name} #{table_records.size} #{table_columns}"
  channel_url = "https://#{CDO.firebase_name}.firebaseio.com/v3/channels/#{encrypted_channel_id}"

  if ILLEGAL_CHAR_REGEX.match(table_name)
    # TODO(dave): remove this line and reenable this block once data blocks / data browser handle illegal characters
    raise "skipping table name containing illegal characters: #{table_name}"
    # legal_table_name = table_name.gsub(ILLEGAL_CHAR_REGEX, '-')
    # puts "renaming table name #{table_name} to #{legal_table_name} in channel #{encrypted_channel_id}"
    # table_name = legal_table_name
  end

  escaped_table_name = URI.escape(table_name, Regexp.new("[^#{URI::PATTERN::UNRESERVED}]"))
  storage_url = "#{channel_url}/storage/tables/#{escaped_table_name}/records.json"
  storage_data = table_records.to_json
  put_with_auth(storage_url, storage_data)

  counters_url = "#{channel_url}/counters/tables/#{escaped_table_name}.json"
  counters_data = {
    lastId: table_records.keys.map(&:to_i).max,
    rowCount: table_records.size
  }.to_json
  put_with_auth(counters_url, counters_data)

  if table_columns
    metadata_url = "#{channel_url}/metadata/tables/#{escaped_table_name}/columns.json"
    put_with_auth(metadata_url, 'null')
    table_columns = ['id'] + (table_columns - ['id'])
    table_columns.each do |column_name|
      post_with_auth(metadata_url, {columnName: column_name}.to_json)
    end
  end
end

# Returns an array of pairs of storage_owner_ids and channel_ids, each representing an applab app.
# There are ~1.1M of these on production.
def fetch_applab_channels
  # In production, use the reporting db to fetch the initial channel list only. This is safe because
  # all new applab projects are using firebase, so any which are missing from this list by
  # definition do not need to be migrated. The check for whether each project is deleted or
  # already migrated is done later against the real db.
  PEGASUS_REPORTING_DB_READONLY[:storage_apps].grep(:value, '%applab%').map do |row|
    [row[:storage_id], row[:id]]
  end
end

# Raises an exception if the list contains duplicates after
# @param names [Array] list of names to detect collisions within
# @param msg [String] message to include in the raised exception
def ensure_no_collisions(list, msg)
  list.combination(2).each do |item1, item2|
    renamed1 = item1.gsub(ILLEGAL_CHAR_REGEX, '-')
    renamed2 = item2.gsub(ILLEGAL_CHAR_REGEX, '-')
    raise "#{msg}: '#{item1}' and '#{item2}' collide after being renamed to '#{renamed1}'" if renamed1 == renamed2
  end
end

def migrate_channel(owner_storage_id, channel_id)
  encrypted_channel_id = storage_encrypt_channel_id(owner_storage_id, channel_id)

  sql_row = PEGASUS_DB[:storage_apps].where(id: channel_id).first
  sql_row_value = JSON.parse(sql_row[:value])
  return if sql_row[:state] == 'deleted' || sql_row_value['useFirebase']

  properties = Hash[DynamoPropertyBag.new(channel_id, nil).to_hash.map {|k, v| [k, v.to_json]}]
  ensure_no_collisions(properties.keys, "property key collision")  unless properties.empty?
  set_firebase_properties(encrypted_channel_id, properties) unless properties.empty?

  table_names = DynamoTable.table_names(channel_id)
  ensure_no_collisions(table_names, "table name collision") unless table_names.empty?
  table_names.each do |table_name|
    dynamo_table = DynamoTable.new(encrypted_channel_id, nil, table_name)
    # A hash from record id to json-encoded records
    table_records = Hash[dynamo_table.to_a.map {|row| [row['id'].to_s, row.to_json]}]
    table_columns = dynamo_table.metadata
    table_columns = JSON.parse(table_columns['column_list']) if table_columns
    set_firebase_table(encrypted_channel_id, table_name, table_records, table_columns)
  end

  # For some reason the local variables row and row_value became mismatched with the channel_id
  # when Parallel.each_with_index was configured to use processes. This doesn't
  # appear to be a problem with threads, but check here anyway to prevent disaster.
  raise "parallelization error: attempting to write data from row id #{sql_row[:id]} to channel id #{channel_id}" if sql_row[:id] != channel_id

  # Mark the project as updated in the Pegasus SQL database
  sql_row_value['migratedToFirebase'] = true
  # TODO(dave): enable this line to perform full migration after successful dry run
  # sql_row_value['useFirebase'] = true
  PEGASUS_DB[:storage_apps].where(id: channel_id).update(value: sql_row_value.to_json)
rescue => e
  puts("Error processing channel id [#{owner_storage_id},#{channel_id}] '#{encrypted_channel_id}': #{e.message}")
  puts e.backtrace
end

def migrate_all
  puts "[#{Time.now}] Initializing DynamoDB client"
  # avoid initializing DynamoDB client many times from different threads
  DynamoPropertyBag.pre_initialize
  DynamoTable.pre_initialize

  puts "[#{Time.now}] Fetching a list of applab apps from Pegasus Reporting DB. This step may take approximately "\
    '15 minutes to run over ~4.5M rows in storage_apps in production...'
  channels = fetch_applab_channels

  puts "[#{Time.now}] Searching for #{channels.length} applab projects in DynamoDB using #{MAX_THREADS} threads..."
  Parallel.each_with_index(channels, in_threads: MAX_THREADS) do |channel_data, channel_index|
    owner_storage_id, channel_id = channel_data
    migrate_channel(owner_storage_id, channel_id)
    puts "[#{Time.now}] #{channel_index} of #{channels.size} projects scanned..." if channel_index % LOG_INCREMENT == 0
  end
  puts "[#{Time.now}] #{channels.size} projects scanned total"
end

def rollback
  puts "'rollback' is for development/emergency purposes only and has been disabled."
  # fetch_applab_channels.each do |_, channel_id|
  #   row = PEGASUS_DB[:storage_apps].where(id: channel_id).first
  #   row_value = JSON.parse(row[:value])
  #   if row_value['migratedToFirebase']
  #     row_value.delete('useFirebase')
  #     row_value.delete('migratedToFirebase')
  #     PEGASUS_DB[:storage_apps].where(id: channel_id).update(value: row_value.to_json)
  #   end
  # end
end

if ARGV[0] == 'all'
  migrate_all
elsif ARGV[0] == 'rollback'
  rollback
elsif ARGV[0] == 'channel'
  encrypted_channel_id = ARGV[1]
  owner_storage_id, channel_id = storage_decrypt_channel_id(encrypted_channel_id)
  migrate_channel(owner_storage_id, channel_id)
else
  puts "Usage:\n"\
    "  migrate all\n"\
    "  migrate rollback\n"\
    "  migrate channel <encrypted-channel-id>"
end
