<%
require 'cdo/aws/dms'
require 'active_support/core_ext/numeric/bytes'
require 'active_support/core_ext/numeric/time'
require 'active_support/inflector'
require 'cdo/redshift'
self.tags.push key: 'environment', value: rack_env
-%>
---
AWSTemplateFormatVersion: 2010-09-09
Description: Data layer including RedShift cluster configuration and synchronization with RDS instance.
# Parameters can be provided via CDO.underscored_parameter, e.g. via locals.yml:
# redshift_password: abcdef
# Parameters are only required for initial stack creation, and reused if not provided on stack update.
Parameters:
  RDSIdentifier:
    Type: String
  # MySQL user must have granted REPLICATION CLIENT and REPLICATION SLAVE permissions.
  # Ref: http://docs.aws.amazon.com/dms/latest/userguide/CHAP_Source.MySQL.html#CHAP_Source.MySQL.Security
  RDSUsername:
    Type: String
  RDSPassword:
    Type: String
    NoEcho: true
  RDSHost:
    Type: String
  # Typically in the form: `cluster-[random-AWS-assigned-string].[region].rds.amazonaws.com` without leading dot.
  RDSClusterEndpointRootDomain:
    Type: String
  ReportingCluster:
    Type: String
  RedshiftDatabase:
    Type: String
    Default: dashboard
  RedshiftUsername:
    Type: String
    Default: dev
  RedshiftPassword:
    Type: String
    NoEcho: true
  RDSBackupAccount:
    Type: String
    NoEcho: true
  LogsBucket:
    Type: String
Resources:
  Tableau:
    Type: AWS::Redshift::Cluster
    DeletionPolicy: Retain
    Properties:
      AllowVersionUpgrade: true
      AutomatedSnapshotRetentionPeriod: 5
      ClusterParameterGroupName: default.redshift-1.0
      ClusterSubnetGroupName: !ImportValue VPC-RedshiftSubnetGroup
      ClusterType: single-node
      ClusterVersion: 1.0
      DBName: {Ref: RedshiftDatabase}
      Encrypted: true
      KmsKeyId: alias/aws/redshift
      MasterUsername: {Ref: RedshiftUsername}
      MasterUserPassword: {Ref: RedshiftPassword}
      NodeType: dc1.large
      PubliclyAccessible: true
      VpcSecurityGroupIds: [!ImportValue VPC-RedshiftSecurityGroup]
  TableauSync:
    Type: AWS::DMS::ReplicationInstance
    Properties:
      AllowMajorVersionUpgrade: true
      AllocatedStorage: 250
      EngineVersion: 3.1.3
      MultiAZ: true
      PubliclyAccessible: false
      ReplicationInstanceClass: dms.c4.4xlarge
      ReplicationInstanceIdentifier: !Ref AWS::StackName
      ReplicationSubnetGroupIdentifier: !ImportValue VPC-DMSSubnetGroup
      VpcSecurityGroupIds: [!ImportValue VPC-DMSSecurityGroup]
  RDSEndpoint:
    Type: AWS::DMS::Endpoint
    Properties:
      EndpointIdentifier: !Sub "RDS-${AWS::StackName}"
      EndpointType: source
      EngineName: mysql
      ServerName: !Ref RDSHost
      Port: 3306
      Username: !Ref RDSUsername
      Password: !Ref RDSPassword
      ExtraConnectionAttributes: ResumeFetchForXRows=0;UnloadTimeout=86400
  AuroraCloneEndpoint:
    Type: AWS::DMS::Endpoint
    Properties:
      EndpointIdentifier: !Sub "AuroraCloneEndpoint-${AWS::StackName}"
      EndpointType: source
      EngineName: aurora
      # At the time this template is rendered the Redshift Import Clone may not exist, so we can't use the RDS describe
      # endpoints API to get this server name.
      ServerName: !Sub "<%= RedshiftImport::CLONE_CLUSTER_ID %>.${RDSClusterEndpointRootDomain}"
      Port: 3306
      Username: !Ref RDSUsername
      Password: !Ref RDSPassword
      ExtraConnectionAttributes: ResumeFetchForXRows=0;UnloadTimeout=86400
  RedshiftEndpoint:
    Type: AWS::DMS::Endpoint
    Properties:
      EndpointIdentifier: !Sub "Redshift-${AWS::StackName}"
      EndpointType: target
      EngineName: redshift
      ServerName: !GetAtt Tableau.Endpoint.Address
      Port: !GetAtt Tableau.Endpoint.Port
      Username: !Ref RedshiftUsername
      Password: !Ref RedshiftPassword
      DatabaseName: !Ref RedshiftDatabase
      # Extra Connection Attributes:
      # https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Introduction.ConnectionAttributes.html#CHAP_Introduction.ConnectionAttributes.Redshift
      ExtraConnectionAttributes: <%={
         # maxFileSize - Specifies the maximum size (in KB) of any CSV file used to transfer data to Amazon Redshift.
         # Default value: 32768 KB (32 MB)
         # Valid values: 1 - 1048576
         # Note: Redshift limit is currently 5 GB according to AWS Support.
         maxFileSize: 5.gigabytes / 1.kilobyte,
         # fileTransferUploadStreams - Specifies the number of threads used to upload a single file.
         # Default value: 10
         # Valid values: 1 - 64
         fileTransferUploadStreams: 20,
         # writeBufferSize - Specifies the size (in KB) of the LOB write buffer.
         # Giving extra memory can speed up the data load process when you have
         # large varchar / text / LOB data to be moved to the destination.
         # Default value: 1000 KB (1 MB)
         writeBufferSize: 8.megabytes / 1.kilobyte
      }.map{|x|x.join('=')}.join(';')%>
<% Cdo::DMS.tasks(aws_dir('dms/tasks.yml')).each do |task_name, table_mappings, schedule| -%>
  DMS<%=task_name.underscore.camelize%>:
    Type: AWS::DMS::ReplicationTask
    Properties:
      MigrationType: full-load
      ReplicationInstanceArn: !Ref TableauSync
      ReplicationTaskIdentifier: <%= task_name %>-task
      SourceEndpointArn: !Ref AuroraCloneEndpoint
      TargetEndpointArn: !Ref RedshiftEndpoint
      ReplicationTaskSettings: !Sub |
        <%= YAML.load(erb_file(aws_dir('dms/replication-task-settings.yml.erb'))).to_json %>
      TableMappings: !Sub |
          <%= table_mappings.to_json %>
      Tags:
        -
          Key: schedule
          Value: <%=schedule%>
<% end -%>
  DMSS3Role:
    Type: AWS::IAM::Role
    Properties:
      RoleName: DMSS3Role
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal: {Service: [dms.amazonaws.com]}
            Action: 'sts:AssumeRole'
      Policies:
        - PolicyName: DMSRolePolicy
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - 's3:PutObject'
                  - 's3:DeleteObject'
                  - 's3:PutObjectTagging'
                  - 's3:ListBucket'
                Resource: 'arn:aws:s3:::cdo-activities-archive*'
      PermissionsBoundary: !ImportValue IAM-DevPermissions
  AuroraS3Role:
    Type: AWS::IAM::Role
    Properties:
      RoleName: AuroraS3Role
      AssumeRolePolicyDocument:
        Statement:
          - Effect: Allow
            Principal: {Service: [rds.amazonaws.com]}
            Action: 'sts:AssumeRole'
      Policies:
        # SELECT INTO OUTFILE S3 permissions:
        # https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/AuroraMySQL.Integrating.Authorizing.IAM.S3CreatePolicy.html
        - PolicyName: SelectIntoOutfileS3
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - 's3:AbortMultipartUpload'
                  - 's3:DeleteObject'
                  - 's3:GetObject'
                  - 's3:ListMultipartUploadParts'
                  - 's3:PutObject'
                Resource: 'arn:aws:s3:::cdo-activities-archive/*'
              - Effect: Allow
                Action:
                  - 's3:ListBucket'
                Resource: 'arn:aws:s3:::cdo-activities-archive'
      PermissionsBoundary: !ImportValue IAM-DevPermissions
<%
  YAML.load(erb_file(aws_dir('cloudformation/db_parameters.yml.erb'))).each do |key, values|
-%>
  <%=key%>DBParameters:
    Type: AWS::RDS::DB<%=key.match('Cluster')%>ParameterGroup
    Properties:
      Description: !Sub "<%=key.titleize%> DB Parameters for ${AWS::StackName}."
      Family: <%=key.include?('Aurora') ? 'aurora-' : ''%>mysql5.7
      Parameters: <%=values.compact.to_json%>
<%end -%>
<%
  # Add CloudWatch Logs Metric Filters for RDS Enhanced Monitoring metrics.
  # Ref: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_Monitoring.OS.html#w2ab1c20c23c17b7b5
  %w(
    diskIO.writeKbPS
    diskIO.readIOsPS
    diskIO.await
    diskIO.readKbPS
    diskIO.rrqmPS
    diskIO.util
    diskIO.avgQueueLen
    diskIO.tps
    diskIO.readKb
    diskIO.writeKb
    diskIO.avgReqSz
    diskIO.wrqmPS
    diskIO.writeIOsPS
    cpuUtilization.total
    cpuUtilization.guest
    cpuUtilization.irq
    cpuUtilization.system
    cpuUtilization.wait
    cpuUtilization.idle
    cpuUtilization.user
    cpuUtilization.total
    cpuUtilization.steal
    cpuUtilization.nice
    tasks.total
    tasks.running
    tasks.blocked
  ).each do |metric|
-%>
  <%=metric.gsub('.', '_').underscore.camelize%>:
    Type: AWS::Logs::MetricFilter
    Properties:
      LogGroupName: RDSOSMetrics
      FilterPattern: "{ $.instanceID = production }"
      MetricTransformations:
        - MetricValue: "$.<%=metric.sub('diskIO', 'diskIO[0]')%>"
          MetricNamespace: "MySQL"
          MetricName: <%=metric.capitalize%>
<% end -%>
  DBSnapshotKey:
    Type: AWS::KMS::Key
    Properties:
      Description: !Sub "Encrypts cross-account DB snapshots for ${AWS::StackName}."
      EnableKeyRotation: true
      KeyPolicy:
        Version: 2012-10-17
        Id: key-policy-1
        Statement:
        - Sid: Enable IAM policies in source account for managing key access.
          Effect: Allow
          Principal: {AWS: [!Sub "arn:aws:iam::${AWS::AccountId}:root"]}
          Action: 'kms:*'
          Resource: '*'
        - Sid: Allow use of the key in backup account.
          Effect: Allow
          Principal: {AWS: [!Sub "arn:aws:iam::${RDSBackupAccount}:root"]}
          Action:
          - 'kms:Encrypt'
          - 'kms:Decrypt'
          - 'kms:ReEncrypt*'
          - 'kms:GenerateDataKey*'
          - 'kms:DescribeKey'
          Resource: '*'
        - Sid: Allow attachment of persistent resources in backup account.
          Effect: Allow
          Principal: {AWS: !Sub "arn:aws:iam::${RDSBackupAccount}:root"}
          Action:
          - 'kms:CreateGrant'
          - 'kms:ListGrants'
          - 'kms:RevokeGrant'
          Resource: '*'
          Condition: {Bool: {'kms:GrantIsForAWSResource': true}}
  DBSnapshotKeyAlias:
    Type: AWS::KMS::Alias
    Properties:
      AliasName: !Sub "alias/snapshot-${AWS::StackName}"
      TargetKeyId: !Ref DBSnapshotKey
# ELB Access Logs tables defined in Glue Data Catalog.
  ELBLogsDatabase:
    Type: AWS::Glue::Database
    Properties:
      DatabaseInput: {Name: elb_logs}
      CatalogId: !Ref AWS::AccountId
  ELBLogsTable:
    Type: AWS::Glue::Table
    Properties:
      DatabaseName: !Ref ELBLogsDatabase
      CatalogId: !Ref AWS::AccountId
      TableInput:
        Name: elb_logs_us_east_1
        TableType: EXTERNAL_TABLE
        Parameters:
          classification: csv
        PartitionKeys:
        # Data is partitioned by year/month/day
        - {Name: year, Type: bigint}
        - {Name: month, Type: bigint}
        - {Name: day, Type: bigint}
        StorageDescriptor:
          Columns: <%=
            # Ref: https://docs.aws.amazon.com/athena/latest/ug/elasticloadbalancer-classic-logs.html#create-elb-table
            str = <<~HIVE
             request_timestamp string,
             elb_name string,
             request_ip string,
             request_port int,
             backend_ip string,
             backend_port int,
             request_processing_time double,
             backend_processing_time double,
             client_response_time double,
             elb_response_code string,
             backend_response_code string,
             received_bytes bigint,
             sent_bytes bigint,
             request_verb string,
             url string,
             protocol string,
             user_agent string,
             ssl_cipher string,
             ssl_protocol string
            HIVE
            str.lines.map{|x| name, type = x.chomp.chomp(',').split(' '); {Name: name, Type: type}}.to_json
          %>
          Location: !Sub 's3://${LogsBucket}/production-codeorg/AWSLogs/${AWS::AccountId}/elasticloadbalancing/${AWS::Region}/'
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          SerdeInfo:
            SerializationLibrary: org.apache.hadoop.hive.serde2.RegexSerDe
            Parameters:
              serialization.format: '1'
              input.regex: '([^ ]*) ([^ ]*) ([^ ]*):([0-9]*) ([^ ]*)[:-]([0-9]*) ([-.0-9]*) ([-.0-9]*) ([-.0-9]*) (|[-0-9]*) (-|[-0-9]*) ([-0-9]*) ([-0-9]*) \"([^ ]*) ([^ ]*) (- |[^ ]*)\" ("[^"]*") ([A-Z0-9-]+) ([A-Za-z0-9.-]*)$'
  ELBLogsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: elb_logs
      Role: !GetAtt GlueRole.Arn
      Description: Update daily partitions generated by ELB Access Logs.
      SchemaChangePolicy:
        UpdateBehavior: LOG
        DeleteBehavior: LOG
      Schedule: {ScheduleExpression: 'cron(0 7 * * ? *)'}
      DatabaseName: !Ref ELBLogsDatabase
      TablePrefix: elb_logs_
      Targets:
        S3Targets:
        - Path: !Sub 's3://${LogsBucket}/production-codeorg/AWSLogs/${AWS::AccountId}/elasticloadbalancing/${AWS::Region}/'
      Configuration: '<%={Version: 1.0, CrawlerOutput: {Partitions: {AddOrUpdateBehavior: 'InheritFromTable'}}}.to_json%>'
  GlueRole:
    Type: AWS::IAM::Role
    Properties:
      <%=service_role 'glue'%>
      Policies:
        - PolicyName: ReadLogs
          PolicyDocument:
            Statement:
              - Effect: Allow
                Action: 's3:GetObject'
                Resource:
                  - !Sub 'arn:aws:s3:::${LogsBucket}/*'
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole'
      PermissionsBoundary: !ImportValue IAM-DevPermissions
  CloudTrailLogsTable:
    Type: AWS::Glue::Table
    Properties:
      DatabaseName: !Ref ELBLogsDatabase
      CatalogId: !Ref AWS::AccountId
      TableInput:
        Name: cloudtrail
        TableType: EXTERNAL_TABLE
        Parameters:
          classification: cloudtrail
        PartitionKeys:
          # Data is partitioned by year/month/day
          - {Name: year, Type: bigint}
          - {Name: month, Type: bigint}
          - {Name: day, Type: bigint}
        StorageDescriptor:
          Location: !Sub 's3://${LogsBucket}/AWSLogs/${AWS::AccountId}/CloudTrail/'
          Columns: <%=
            # Ref: https://docs.aws.amazon.com/athena/latest/ug/cloudtrail-logs.html#create-cloudtrail-table
            str = <<~HIVE
            eventversion STRING,
            useridentity STRUCT<type:STRING,principalid:STRING,arn:STRING,accountid:STRING,invokedby:STRING,accesskeyid:STRING,userName:STRING,sessioncontext:STRUCT<attributes:STRUCT<mfaauthenticated:STRING,creationdate:STRING>,sessionissuer:STRUCT<type:STRING,principalId:STRING,arn:STRING,accountId:STRING,userName:STRING>>>,
            eventtime STRING,
            eventsource STRING,
            eventname STRING,
            awsregion STRING,
            sourceipaddress STRING,
            useragent STRING,
            errorcode STRING,
            errormessage STRING,
            requestparameters STRING,
            responseelements STRING,
            additionaleventdata STRING,
            requestid STRING,
            eventid STRING,
            resources ARRAY<STRUCT<ARN:STRING,accountId:STRING,type:STRING>>,
            eventtype STRING,
            apiversion STRING,
            readonly STRING,
            recipientaccountid STRING,
            serviceeventdetails STRING,
            sharedeventid STRING,
            vpcendpointid STRING
            HIVE
            str.lines.map {|x| name, type = x.chomp.chomp(',').split(' '); {Name: name, Type: type.downcase}}.to_json
          %>
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          InputFormat: com.amazon.emr.cloudtrail.CloudTrailInputFormat
          SerdeInfo:
            SerializationLibrary: com.amazon.emr.hive.serde.CloudTrailSerde
            Parameters:
              serialization.format: '1'
  CloudTrailLogsCrawler:
    Type: AWS::Glue::Crawler
    Properties:
      Name: cloudtrail_logs
      Role: !GetAtt GlueRole.Arn
      Description: Update daily partitions generated by CloudTrail Logs.
      SchemaChangePolicy:
        UpdateBehavior: LOG
        DeleteBehavior: LOG
      Schedule: {ScheduleExpression: 'cron(0 7 * * ? *)'}
      DatabaseName: !Ref ELBLogsDatabase
      TablePrefix: cloudtrail_logs_
      Targets:
        S3Targets:
          - Path: !Sub 's3://${LogsBucket}/AWSLogs/${AWS::AccountId}/CloudTrail/'
      Configuration: '<%={Version: 1.0, CrawlerOutput: {Partitions: {AddOrUpdateBehavior: 'InheritFromTable'}}}.to_json%>'
  CloudFrontLogsTable:
    Type: AWS::Glue::Table
    Properties:
      DatabaseName: !Ref ELBLogsDatabase
      CatalogId: !Ref AWS::AccountId
      TableInput:
        Name: cloudfront_logs
        TableType: EXTERNAL_TABLE
        Parameters:
          classification: csv
          compressionType: gzip
          skip.header.line.count: '2'
        PartitionKeys:
        # Data is partitioned by year/month/day/hour
        - {Name: year, Type: bigint}
        - {Name: month, Type: bigint}
        - {Name: day, Type: bigint}
        - {Name: hour, Type: bigint}
        StorageDescriptor:
          Location: !Sub 's3://${LogsBucket}/cloudfront/<%=environment%>-dashboard-cdn/'
          Columns: <%=
            # Ref: https://docs.aws.amazon.com/athena/latest/ug/cloudfront-logs.html#create-cloudfront-table
            str = <<~HIVE
              date DATE,
              time STRING,
              location STRING,
              bytes BIGINT,
              requestip STRING,
              method STRING,
              host STRING,
              uri STRING,
              status INT,
              referrer STRING,
              useragent STRING,
              querystring STRING,
              cookie STRING,
              resulttype STRING,
              requestid STRING,
              hostheader STRING,
              requestprotocol STRING,
              requestbytes BIGINT,
              timetaken FLOAT,
              xforwardedfor STRING,
              sslprotocol STRING,
              sslcipher STRING,
              responseresulttype STRING,
              httpversion STRING,
              filestatus STRING,
              encryptedfields INT
            HIVE
            str.lines.map{|x| name, type = x.chomp.chomp(',').split(' '); {Name: name, Type: type.downcase}}.to_json
          %>
          OutputFormat: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
          InputFormat: org.apache.hadoop.mapred.TextInputFormat
          SerdeInfo:
            SerializationLibrary: "org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe"
            Parameters:
              serialization.format: "\t"
              field.delim: "\t"
  CdoLogsBucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties:
      Bucket: !Ref LogsBucket
      PolicyDocument:
        Statement:
        - Sid: LogsBucketNotification
          Effect: Allow
          Action: 's3:PutBucketNotification'
          Resource: !Sub "arn:aws:s3:::${LogsBucket}"
          Principal: {AWS: !ImportValue S3LogsRole}
        - Sid: ELBAccessLogsWrite
          Effect: Allow
          Principal: {AWS: "arn:aws:iam::127311923021:root"}
          Action: 's3:PutObject'
          Resource: !Sub "arn:aws:s3:::${LogsBucket}/AWSLogs/${AWS::AccountId}/*"
        - Sid: AWSCloudTrailAclCheck
          Effect: Allow
          Principal: {Service: cloudtrail.amazonaws.com}
          Action: 's3:GetBucketAcl'
          Resource: !Sub "arn:aws:s3:::${LogsBucket}"
        - Sid: AWSCloudTrailWrite
          Effect: Allow
          Principal: {Service: cloudtrail.amazonaws.com}
          Action: 's3:PutObject'
          Resource: !Sub "arn:aws:s3:::${LogsBucket}/AWSLogs/${AWS::AccountId}/*"
          Condition:
            StringEquals:
              "s3:x-amz-acl": bucket-owner-full-control
  S3PartitionCloudFrontLogPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: S3PartitionCloudFrontLog
      Principal: s3.amazonaws.com
      SourceAccount: !Ref "AWS::AccountId"
      SourceArn: !Sub "arn:aws:s3:::${LogsBucket}"
  CdoLogsBucketConfiguration:
    Type: Custom::S3BucketConfiguration
    DependsOn:
    - S3PartitionCloudFrontLogPermission
    - CdoLogsBucketPolicy
    Properties:
      ServiceToken: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:S3BucketConfiguration'
      Bucket: !Ref LogsBucket
      NotificationConfiguration:
        LambdaFunctionConfigurations:
        - Events: ['s3:ObjectCreated:*']
          LambdaFunctionArn: !Sub 'arn:aws:lambda:${AWS::Region}:${AWS::AccountId}:function:S3PartitionCloudFrontLog'
          Filter:
            Key:
              FilterRules:
              - Name: prefix
                Value: <%=environment%>-dashboard-cdn/
              - Name: suffix
                Value: .gz
