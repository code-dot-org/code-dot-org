---
# Replication task settings template
# Reference documentation:
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.html

# Error handling behavior of your replication task during change data capture (CDC).
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ErrorHandling.html
ErrorBehavior:
  # [undocumented setting]
  FullLoadIgnoreConflicts: true
  # The maximum number of attempts made to restart a task when an environmental error occurs.
  # After the system attempts to restart the task the designated number of times,
  # the task is stopped and manual intervention is required.
  # Set this value to -1 to attempt a restart six times.
  # Set this value to 0 to never attempt to restart a task.
  # The default is 1.
  RecoverableErrorCount: -1
  #  The number of seconds that AWS DMS waits between attempts to restart a task.
  # The default is 5.
  RecoverableErrorInterval: 5
  # When enabled, the interval between attempts to restart a task is increased each time a restart is attempted.
  # The default is true.
  RecoverableErrorThrottling: true
  # The maximum number of seconds that AWS DMS waits between attempts to restart a task if RecoverableErrorThrottling is enabled.
  # The default is 1800.
  RecoverableErrorThrottlingMax: 1800
  # The maximum number of errors that can occur to the general data or metadata for a specific table.
  # When this number is reached, the data for the table is handled according to the policy set in the TableErrorEscalationPolicy.
  # The default is 0.
  TableErrorEscalationCount: 50
  # Determines the action AWS DMS takes when the maximum number of errors (set using the TableErrorEscalationCount parameter).
  # The default and only user setting is STOP_TASK, where the task is stopped and manual intervention is required.
  TableErrorEscalationPolicy: STOP_TASK
  # Determines the action AWS DMS takes when an error occurs when processing data or metadata for a specific table.
  # This error only applies to general table data and is not an error that relates to a specific record.
  # The default is SUSPEND_TABLE.
  TableErrorPolicy: SUSPEND_TABLE
  # Sets the maximum number of errors that can occur to the data for a specific record.
  # When this number is reached, the data for the table that contains the error record is handled
  # according to the policy set in the DataErrorEscalationCount.
  # The default is 0.
  DataErrorEscalationCount: 50
  # Determines the action AWS DMS takes when the maximum number of errors (set in the DataErrorsEscalationCount parameter) is reached.
  # The default is SUSPEND_TABLE.
  DataErrorEscalationPolicy: SUSPEND_TABLE
  # Determines the action AWS DMS takes when there is an error related to data processing at the record level.
  # Some examples of data processing errors include conversion errors, errors in transformation, and bad data.
  # The default is LOG_ERROR.
  DataErrorPolicy: LOG_ERROR
  # Determines the action AWS DMS takes when data is truncated.
  # The default is LOG_ERROR.
  DataTruncationErrorPolicy: LOG_ERROR
  # Determines what action AWS DMS takes when there is a conflict with a DELETE operation.
  # The default is IGNORE_RECORD.
  ApplyErrorDeletePolicy: IGNORE_RECORD
  # Determines what action AWS DMS takes when there is a conflict with an INSERT operation.
  # The default is LOG_ERROR.
  ApplyErrorInsertPolicy: LOG_ERROR
  # Determines what action AWS DMS takes when there is a conflict with an UPDATE operation.
  # The default is LOG_ERROR.
  ApplyErrorUpdatePolicy: LOG_ERROR
  # Determines what action AWS DMS takes when the maximum number of errors (set using the ApplyErrorsEscalationCount parameter) is reached.
  ApplyErrorEscalationPolicy: LOG_ERROR
  # Sets the maximum number of APPLY conflicts that can occur for a specific table during a change process operation.
  # When this number is reached, the data for the table is handled according to the policy set in the ApplyErrorEscalationPolicy parameter.
  # The default is 0.
  ApplyErrorEscalationCount: 0
  # Set this to true to cause the task to fail when a truncation is performed on any of the tracked tables during CDC.
  # The failure message will be: "Truncation DDL detected."
  # The default is false.
  ApplyErrorFailOnTruncationDdl: false
  # This option applies to tasks using Oracle as a source with CDC.
  # Set this to true to cause a task to fail when a transaction is open for more time than the specified timeout and could be dropped.
  FailOnTransactionConsistencyBreached: false
  # Set this to true to cause a task to fail when the transformation rules defined for a task find no tables when the task starts.
  # The default is false.
  FailOnNoTablesCaptured: false

# How AWS DMS handles DDL changes for target tables during change data capture (CDC).
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.DDLHandling.html
ChangeProcessingDdlHandlingPolicy:
  # Set this option to true to alter the target table when the source table is altered.
  HandleSourceTableAltered: true
  # Set this option to true to truncate the target table when the source table is truncated.
  HandleSourceTableTruncated: true
  #  Set this option to true to drop the target table when the source table is dropped.
  HandleSourceTableDropped: true

# How AWS DMS handles changes for target tables during change data capture (CDC).
# Several of these settings depend on the value of the target metadata parameter BatchApplyEnabled.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ChangeProcessingTuning.html
ChangeProcessingTuning:
  # If set to true, transactional integrity is preserved
  # and a batch is guaranteed to contain all the changes within a transaction from the source.
  # The default value is true.
  # This setting applies only to Oracle target endpoints.
  BatchApplyPreserveTransaction: true
  # Sets the minimum amount of time in seconds that AWS DMS waits between each application of batch changes.
  # The default value is 1.
  BatchApplyTimeoutMin: <%= 1.hour.to_i %>
  # Sets the maximum amount of time in seconds that AWS DMS waits between each application of batch changes before timing out.
  # The default value is 30.
  BatchApplyTimeoutMax: <%= 1.hour.to_i %>
  # Sets the maximum amount of memory in (MB) to use for pre-processing in Batch optimized apply mode.
  # The default value is 500.
  BatchApplyMemoryLimit: 500
  # Sets the number of changes applied in a single change processing statement.
  # Select the check box and then optionally change the default value.
  # The default value is 10,000. A value of 0 means there is no limit applied.
  BatchSplitSize: 0
  # Sets the minimum number of changes to include in each transaction.
  # The default value is 1000.
  MinTransactionSize: 1000
  # Sets the maximum time in seconds for AWS DMS to collect transactions in batches before declaring a timeout.
  # The default value is 1.
  CommitTimeout: 1
  # Sets the maximum size (in MB) that all transactions can occupy in memory before being written to disk.
  # The default value is 1024.
  MemoryLimitTotal: 1024
  # Sets the maximum time in seconds that each transaction can stay in memory before being written to disk.
  # The duration is calculated from the time that AWS DMS started capturing the transaction.
  # The default value is 60.
  MemoryKeepTime: <%= 10.minutes.to_i %>
  # Sets the maximum number of prepared statements to store on the server for later execution when applying changes to the target.
  # The default value is 50. The maximum value is 200.
  StatementCacheSize: 50

# Stream buffer settings.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.StreamBuffer.html
StreamBufferSettings:
  # Use this option to set the size of the control stream buffer.
  # Value is in MB, and can be from 1 to 8.
  # The default value is 5.
  # You may need to increase this when working with a very large number of tables, such as tens of thousands of tables.
  CtrlStreamBufferSizeInMB: 5
  # Use this option to indicate the maximum size of each data stream buffer.
  # The default size is 8 MB.
  # You might need to increase the value for this option when you work with very large LOBs
  # or if you receive a message in the log files stating that the stream buffer size is insufficient.
  # When calculating the size of this option you can use the following equation:
  # [Max LOB size (or LOB chunk size)]*[number of LOB columns]*[number of stream buffers]*[number of tables loading in parallel per task(MaxFullLoadSubTasks)]*3
  StreamBufferSizeInMB: 8
  # Use this option to specify the number of data stream buffers for the migration task.
  # The default stream buffer number is 3.
  # Increasing the value of this setting may increase the speed of data extraction.
  # However, this performance increase is highly dependent on the migration environment,
  # including the source system and instance class of the replication server.
  # The default is sufficient for most situations.
  StreamBufferCount: 3

# Control tables provide information about the AWS DMS task,
# as well as useful statistics that you can use to plan and manage both the current migration task and future tasks.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.ControlTable.html
ControlTablesSettings:
  # Replication Status (dmslogs.awsdms_status)
  # This table provides details about the current task including task status, amount of memory consumed by the task,
  # number of changes not yet applied to the target, and the position in the source database from which AWS DMS is currently reading.
  # It also indicates if the task is a full load or change data capture (CDC).
  StatusTableEnabled: true
  # Suspended Tables (dmslogs.awsdms_suspended_tables)
  # This table provides a list of suspended tables as well as the reason they were suspended.
  SuspendedTablesTableEnabled: true
  # Replication History (dmslogs.awsdms_history)
  # This table provides information about the replication history including the number and volume of records processed during the task,
  # latency at the end of a CDC task, and other statistics.
  HistoryTableEnabled: true
  # Use this option to indicate the database schema name for the AWS DMS target Control Tables.
  # If you do not enter any information in this field, then the tables are copied to the default location in the database.
  ControlSchema: ""
  # Use this option to indicate the length of each time slot in the Replication History table. The default is 5 minutes.
  HistoryTimeslotInMinutes: 5
  historyTimeslotInMinutes: 5 # this alternate-spelling property is auto-generated by task creation [bug/typo in API?].

# Specify which component activities are logged and what amount of information is written to the log.
# The logging feature uses Amazon CloudWatch to log information during the migration process.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.Logging.html
Logging:
  # Enable CloudWatch logging.
  EnableLogging: true
  # Once you specify a component activity, you can then specify the amount of information that is logged.
  # The following list is in order from the lowest level of information to the highest level of information.
  # The higher levels always include information from the lower levels.
  # These severity values include:
  # LOGGER_SEVERITY_ERROR - Error messages are written to the log.
  # LOGGER_SEVERITY_WARNING - Warnings and error messages are written to the log.
  # LOGGER_SEVERITY_INFO - Informational messages, warnings, and error messages are written to the log.
  # LOGGER_SEVERITY_DEFAULT - Debug messages, informational messages, warnings, and error messages are written to the log.
  # LOGGER_SEVERITY_DEBUG - Debug messages, informational messages, warnings, and error messages are written to the log.
  # LOGGER_SEVERITY_DETAILED_DEBUG - All information is written to the log.
  LogComponents:
  # Data is unloaded from the source database.
  - {Id: SOURCE_UNLOAD,  Severity: LOGGER_SEVERITY_WARNING}
  # Data is captured from the source database.
  - {Id: SOURCE_CAPTURE, Severity: LOGGER_SEVERITY_WARNING}
  # Data is loaded into the target database.
  - {Id: TARGET_LOAD,    Severity: LOGGER_SEVERITY_WARNING}
  # Data and DDL are applied to the target database.
  - {Id: TARGET_APPLY,   Severity: LOGGER_SEVERITY_WARNING}
  # The task manager triggers an event.
  - {Id: TASK_MANAGER,   Severity: LOGGER_SEVERITY_WARNING}

# Full load settings.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.FullLoad.html
FullLoadSettings:
  # To indicate how to handle loading the target at full-load startup, specify one of the following values:
  # DO_NOTHING - Data and metadata of the existing target table are not affected.
  # DROP_AND_CREATE - The existing table is dropped and a new table is created in its place.
  # TRUNCATE_BEFORE_LOAD - Data is truncated without affecting the table metadata.
  TargetTablePrepMode: DROP_AND_CREATE
  # To delay primary key or unique index creation until after full load completes, set the CreatePkAfterFullLoad option.
  # When this option is selected, you cannot resume incomplete full load tasks.
  CreatePkAfterFullLoad: false
  # Set this option to true to stop a task after a full load completes and cached changes are applied.
  StopTaskCachedChangesApplied: false
  # Set this option to true to stop a task before cached changes are applied.
  StopTaskCachedChangesNotApplied: false
  #  Set this option to indicate the maximum number of tables to load in parallel.
  # The default is 8; the maximum value is 50.
  MaxFullLoadSubTasks: 20
  # To set the number of seconds that AWS DMS waits for transactions to close before beginning a full-load operation,
  # if transactions are open when the task starts, set the TransactionConsistencyTimeout option.
  # The default value is 600 (10 minutes).
  # AWS DMS begins the full load after the timeout value is reached, even if there are open transactions.
  TransactionConsistencyTimeout: 600
  # To indicate the maximum number of events that can be transferred together, set the CommitRate option.
  CommitRate: 10000

# Target metadata settings.
# https://docs.aws.amazon.com/dms/latest/userguide/CHAP_Tasks.CustomizingTasks.TaskSettings.TargetMetadata.html
TargetMetadata:
  # Determines if each transaction is applied individually or if changes are committed in batches.
  # The default value is false.
  # If set to true, AWS DMS commits changes in batches by a pre-processing action that groups
  # the transactions into batches in the most efficient way.
  # Setting this value to true can affect transactional integrity, so you must select BatchApplyPreserveTransaction
  # in the ChangeProcessingTuning section to specify how the system handles referential integrity issues.
  BatchApplyEnabled: true
  # An option for PostgreSQL and MySQL target endpoints that defines the maximum size on disk of stored,
  # unloaded data, such as .csv files.
  # This option overrides the connection attribute.
  # You can provide values from 0, which indicates that this option doesn't override the connection attribute, to 100,000 KB.
  LoadMaxFileSize: 0

  # If you set SupportLobs=true, you must set one of the following [LimitedSizeLobMode or FullLobMode] to true:
  SupportLobs: true

  # In limited LOB mode, you set a maximum size LOB that AWS DMS should accept.
  # Doing so allows AWS DMS to pre-allocate memory and load the LOB data in bulk.
  # LOBs that exceed the maximum LOB size are truncated and a warning is issued to the log file.
  # In limited LOB mode you get significant performance gains over full LOB mode.
  # We recommend that you use limited LOB mode whenever possible.
  LimitedSizeLobMode: true
  # Enter the maximum size, in kilobytes, for an individual LOB.
  # When a task is configured to run in limited LOB mode, the Max LOB size (K) option sets the maximum size LOB that AWS DMS accepts.
  # Any LOBs that are larger than this value will be truncated to this value.
  LobMaxSize: 32

  # In full LOB mode AWS DMS migrates all LOBs from source to target regardless of size.
  # In this configuration, AWS DMS has no information about the maximum size of LOBs to expect.
  # Thus, LOBs are migrated one at a time, piece by piece.
  # Full LOB mode can be quite slow.
  FullLobMode: false
  # Enter the size, in kilobytes, of the LOB chunks to use when replicating the data to the target.
  # The FullLobMode option works best for very large LOB sizes but tends to cause slower loading.
  #
  # When a task is configured to use full LOB mode, AWS DMS retrieves LOBs in pieces.
  # The LOB chunk size (K) option determines the size of each piece.
  # When setting this option, pay particular attention to the maximum packet size allowed by your network configuration.
  # If the LOB chunk size exceeds your maximum allowed packet size, you might see disconnect errors.
  LobChunkSize: 0

  # The target table schema name. If this metadata option is empty, the schema from the source table is used.
  # AWS DMS automatically adds the owner prefix for the target database to all tables if no source schema is defined.
  # This option should be left empty for MySQL-type target endpoints.
  TargetSchema: ""
  # Specifies the number of threads AWS DMS uses to load each table into the target database.
  # The maximum value for a MySQL target is 16; the maximum value for a DynamoDB target is 32.
  # The maximum limit can be increated upon request.
  ParallelLoadThreads: 0
  # Specifies the maximum number of records to store in the buffer used by the parallel load threads to load data to the target.
  # The default value is 50. Maximum value is 1000.
  # This field is currently only valid when DynamoDB is the target.
  # This parameter should be used in conjunction with ParallelLoadThreads and is valid only when ParallelLoadThreads > 1.
  ParallelLoadBufferSize: 0
